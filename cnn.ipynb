{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from __future__ import division\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"data_content\", header=None, skip_blank_lines=False)\n",
    "label = pd.read_table(\"label\", header=None, dtype='category', skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns = ['question']\n",
    "label.columns = ['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67213, 1)\n",
      "(67213, 1)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 0.2\n",
    "# Separate each muscle group data\n",
    "chest = train[label['group']=='Chest']\n",
    "chest_label = label[label['group']=='Chest']\n",
    "chest_len = int(chest.shape[0]*ratio)\n",
    "\n",
    "shoulder = train[label['group']=='Shoulders']\n",
    "shoulder_len = int(shoulder.shape[0]*ratio)\n",
    "shoulder_label = label[label['group']=='Shoulders']\n",
    "\n",
    "back = train[label['group']=='Back']\n",
    "back_label = label[label['group']=='Back']\n",
    "back_len = int(back.shape[0]*ratio)\n",
    "\n",
    "leg = train[label['group']=='Leg']\n",
    "leg_label = label[label['group']=='Leg']\n",
    "leg_len = int(leg.shape[0]*ratio)\n",
    "\n",
    "tricep = train[label['group']=='Triceps']\n",
    "tricep_label = label[label['group']=='Triceps']\n",
    "tricep_len = int(tricep.shape[0]*ratio)\n",
    "\n",
    "bicep = train[label['group']=='Biceps']\n",
    "bicep_label = label[label['group']=='Biceps']\n",
    "bicep_len = int(bicep.shape[0]*ratio)\n",
    "\n",
    "ab = train[label['group']=='Abs']\n",
    "ab_label = label[label['group']=='Abs']\n",
    "ab_len = int(ab.shape[0]*ratio)\n",
    "\n",
    "glute = train[label['group']=='Glutes']\n",
    "glute_label = label[label['group']=='Glutes']\n",
    "glute_len = int(glute.shape[0]*ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13440, 1)\n",
      "(13440, 1)\n"
     ]
    }
   ],
   "source": [
    "# Take only the top ratio% of each group of data\n",
    "train_data = pd.concat([chest[0:chest_len], shoulder[0:shoulder_len], back[0:back_len], \n",
    "                        leg[0:leg_len], tricep[0:tricep_len], bicep[0:bicep_len], ab[0:ab_len], glute[0: glute_len]])\n",
    "print train_data.shape\n",
    "train_label = pd.concat([chest_label[0:chest_len], shoulder_label[0:shoulder_len], back_label[0:back_len], \n",
    "                   leg_label[0:leg_len], tricep_label[0:tricep_len], bicep_label[0:bicep_len], ab_label[0:ab_len],\n",
    "                         glute_label[0: glute_len]])\n",
    "print train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_label = pd.Series(train_label['group'],dtype='category')\n",
    "y_label.cat.categories \n",
    "y_label.cat.categories = [0,1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#label binizer\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([0,1,2,3,4,5,6,7])\n",
    "label = lb.transform(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize TFIDF vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),stop_words=\"english\")\n",
    "data = vectorizer.fit_transform(train_data['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduce features using SVD\n",
    "SVD = TruncatedSVD(n_components=100, n_iter=5, random_state=0)\n",
    "train = SVD.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10752, 100)\n",
      "(10752, 8)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "train_data, testX, train_label, testY = train_test_split(train, label, test_size=0.2, random_state=0)\n",
    "print train_data.shape\n",
    "print train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow setup\n",
    "numFeatures = train_data.shape[1]\n",
    "numLabels = 8\n",
    "numEpochs = 2000\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.005,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=train_data.shape[0],\n",
    "                                          decay_rate= 0.9,\n",
    "                                          staircase=True)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "Y = tf.placeholder(tf.float32, [None, numLabels])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=(np.sqrt(6/numFeatures+\n",
    "                                                         numLabels+1)),\n",
    "                                       name=\"weights\"))\n",
    "\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=(np.sqrt(6/numFeatures+numLabels+1)),\n",
    "                                    name=\"bias\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow operation\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\") \n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-Y, name=\"squared_error_cost\")\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.0998884\n",
      "step 0, cost 10241.2\n",
      "step 0, change in cost 10241.2\n",
      "step 10, training accuracy 0.215309\n",
      "step 10, cost 5036.02\n",
      "step 10, change in cost 5205.21\n",
      "step 20, training accuracy 0.218564\n",
      "step 20, cost 5018.44\n",
      "step 20, change in cost 17.5718\n",
      "step 30, training accuracy 0.220796\n",
      "step 30, cost 5001.96\n",
      "step 30, change in cost 16.48\n",
      "step 40, training accuracy 0.22247\n",
      "step 40, cost 4985.9\n",
      "step 40, change in cost 16.0693\n",
      "step 50, training accuracy 0.224609\n",
      "step 50, cost 4970.14\n",
      "step 50, change in cost 15.7593\n",
      "step 60, training accuracy 0.22526\n",
      "step 60, cost 4954.7\n",
      "step 60, change in cost 15.4321\n",
      "step 70, training accuracy 0.227679\n",
      "step 70, cost 4939.64\n",
      "step 70, change in cost 15.0684\n",
      "step 80, training accuracy 0.229725\n",
      "step 80, cost 4925.02\n",
      "step 80, change in cost 14.6123\n",
      "step 90, training accuracy 0.231213\n",
      "step 90, cost 4910.94\n",
      "step 90, change in cost 14.083\n",
      "step 100, training accuracy 0.232608\n",
      "step 100, cost 4897.43\n",
      "step 100, change in cost 13.5098\n",
      "step 110, training accuracy 0.232794\n",
      "step 110, cost 4884.53\n",
      "step 110, change in cost 12.8984\n",
      "step 120, training accuracy 0.234375\n",
      "step 120, cost 4872.18\n",
      "step 120, change in cost 12.3496\n",
      "step 130, training accuracy 0.235677\n",
      "step 130, cost 4860.33\n",
      "step 130, change in cost 11.8481\n",
      "step 140, training accuracy 0.236235\n",
      "step 140, cost 4848.91\n",
      "step 140, change in cost 11.4287\n",
      "step 150, training accuracy 0.237537\n",
      "step 150, cost 4837.83\n",
      "step 150, change in cost 11.0776\n",
      "step 160, training accuracy 0.239025\n",
      "step 160, cost 4827.02\n",
      "step 160, change in cost 10.8066\n",
      "step 170, training accuracy 0.239397\n",
      "step 170, cost 4816.46\n",
      "step 170, change in cost 10.5615\n",
      "step 180, training accuracy 0.239862\n",
      "step 180, cost 4806.16\n",
      "step 180, change in cost 10.3018\n",
      "step 190, training accuracy 0.240234\n",
      "step 190, cost 4796.17\n",
      "step 190, change in cost 9.99121\n",
      "step 200, training accuracy 0.240885\n",
      "step 200, cost 4786.6\n",
      "step 200, change in cost 9.57031\n",
      "step 210, training accuracy 0.24135\n",
      "step 210, cost 4777.56\n",
      "step 210, change in cost 9.03418\n",
      "step 220, training accuracy 0.241722\n",
      "step 220, cost 4769.12\n",
      "step 220, change in cost 8.44727\n",
      "step 230, training accuracy 0.242094\n",
      "step 230, cost 4761.23\n",
      "step 230, change in cost 7.88525\n",
      "step 240, training accuracy 0.242188\n",
      "step 240, cost 4753.89\n",
      "step 240, change in cost 7.34473\n",
      "step 250, training accuracy 0.242746\n",
      "step 250, cost 4747.01\n",
      "step 250, change in cost 6.87354\n",
      "step 260, training accuracy 0.242839\n",
      "step 260, cost 4740.54\n",
      "step 260, change in cost 6.4668\n",
      "step 270, training accuracy 0.242653\n",
      "step 270, cost 4734.45\n",
      "step 270, change in cost 6.09473\n",
      "step 280, training accuracy 0.242281\n",
      "step 280, cost 4728.68\n",
      "step 280, change in cost 5.7749\n",
      "step 290, training accuracy 0.242932\n",
      "step 290, cost 4723.19\n",
      "step 290, change in cost 5.49023\n",
      "step 300, training accuracy 0.243211\n",
      "step 300, cost 4717.95\n",
      "step 300, change in cost 5.23975\n",
      "step 310, training accuracy 0.243025\n",
      "step 310, cost 4712.94\n",
      "step 310, change in cost 5.00293\n",
      "step 320, training accuracy 0.243397\n",
      "step 320, cost 4708.15\n",
      "step 320, change in cost 4.79004\n",
      "step 330, training accuracy 0.24349\n",
      "step 330, cost 4703.55\n",
      "step 330, change in cost 4.60156\n",
      "step 340, training accuracy 0.243862\n",
      "step 340, cost 4699.13\n",
      "step 340, change in cost 4.41797\n",
      "step 350, training accuracy 0.243769\n",
      "step 350, cost 4694.87\n",
      "step 350, change in cost 4.25977\n",
      "step 360, training accuracy 0.243769\n",
      "step 360, cost 4690.77\n",
      "step 360, change in cost 4.09961\n",
      "step 370, training accuracy 0.244327\n",
      "step 370, cost 4686.81\n",
      "step 370, change in cost 3.96094\n",
      "step 380, training accuracy 0.244513\n",
      "step 380, cost 4682.99\n",
      "step 380, change in cost 3.82715\n",
      "step 390, training accuracy 0.244978\n",
      "step 390, cost 4679.29\n",
      "step 390, change in cost 3.69775\n",
      "step 400, training accuracy 0.245443\n",
      "step 400, cost 4675.7\n",
      "step 400, change in cost 3.58838\n",
      "step 410, training accuracy 0.245629\n",
      "step 410, cost 4672.22\n",
      "step 410, change in cost 3.47461\n",
      "step 420, training accuracy 0.245815\n",
      "step 420, cost 4668.85\n",
      "step 420, change in cost 3.37207\n",
      "step 430, training accuracy 0.245815\n",
      "step 430, cost 4665.58\n",
      "step 430, change in cost 3.27539\n",
      "step 440, training accuracy 0.245815\n",
      "step 440, cost 4662.39\n",
      "step 440, change in cost 3.18848\n",
      "step 450, training accuracy 0.245722\n",
      "step 450, cost 4659.29\n",
      "step 450, change in cost 3.09717\n",
      "step 460, training accuracy 0.245815\n",
      "step 460, cost 4656.28\n",
      "step 460, change in cost 3.01416\n",
      "step 470, training accuracy 0.246373\n",
      "step 470, cost 4653.34\n",
      "step 470, change in cost 2.93652\n",
      "step 480, training accuracy 0.246373\n",
      "step 480, cost 4650.47\n",
      "step 480, change in cost 2.86719\n",
      "step 490, training accuracy 0.246466\n",
      "step 490, cost 4647.67\n",
      "step 490, change in cost 2.7998\n",
      "step 500, training accuracy 0.246466\n",
      "step 500, cost 4644.95\n",
      "step 500, change in cost 2.72852\n",
      "step 510, training accuracy 0.24628\n",
      "step 510, cost 4642.28\n",
      "step 510, change in cost 2.66602\n",
      "step 520, training accuracy 0.246559\n",
      "step 520, cost 4639.67\n",
      "step 520, change in cost 2.60791\n",
      "step 530, training accuracy 0.247024\n",
      "step 530, cost 4637.12\n",
      "step 530, change in cost 2.5542\n",
      "step 540, training accuracy 0.247303\n",
      "step 540, cost 4634.62\n",
      "step 540, change in cost 2.50146\n",
      "step 550, training accuracy 0.247861\n",
      "step 550, cost 4632.17\n",
      "step 550, change in cost 2.44678\n",
      "step 560, training accuracy 0.24814\n",
      "step 560, cost 4629.77\n",
      "step 560, change in cost 2.40332\n",
      "step 570, training accuracy 0.248512\n",
      "step 570, cost 4627.42\n",
      "step 570, change in cost 2.34619\n",
      "step 580, training accuracy 0.248791\n",
      "step 580, cost 4625.11\n",
      "step 580, change in cost 2.30908\n",
      "step 590, training accuracy 0.248605\n",
      "step 590, cost 4622.85\n",
      "step 590, change in cost 2.26514\n",
      "step 600, training accuracy 0.248605\n",
      "step 600, cost 4620.62\n",
      "step 600, change in cost 2.2207\n",
      "step 610, training accuracy 0.248605\n",
      "step 610, cost 4618.45\n",
      "step 610, change in cost 2.1792\n",
      "step 620, training accuracy 0.248512\n",
      "step 620, cost 4616.3\n",
      "step 620, change in cost 2.14941\n",
      "step 630, training accuracy 0.248419\n",
      "step 630, cost 4614.19\n",
      "step 630, change in cost 2.10938\n",
      "step 640, training accuracy 0.248419\n",
      "step 640, cost 4612.12\n",
      "step 640, change in cost 2.07129\n",
      "step 650, training accuracy 0.248698\n",
      "step 650, cost 4610.07\n",
      "step 650, change in cost 2.0415\n",
      "step 660, training accuracy 0.248884\n",
      "step 660, cost 4608.07\n",
      "step 660, change in cost 2.00684\n",
      "step 670, training accuracy 0.249163\n",
      "step 670, cost 4606.09\n",
      "step 670, change in cost 1.97607\n",
      "step 680, training accuracy 0.249721\n",
      "step 680, cost 4604.14\n",
      "step 680, change in cost 1.95117\n",
      "step 690, training accuracy 0.249628\n",
      "step 690, cost 4602.23\n",
      "step 690, change in cost 1.90625\n",
      "step 700, training accuracy 0.249628\n",
      "step 700, cost 4600.34\n",
      "step 700, change in cost 1.89307\n",
      "step 710, training accuracy 0.249628\n",
      "step 710, cost 4598.48\n",
      "step 710, change in cost 1.85742\n",
      "step 720, training accuracy 0.249349\n",
      "step 720, cost 4596.65\n",
      "step 720, change in cost 1.83057\n",
      "step 730, training accuracy 0.249349\n",
      "step 730, cost 4594.84\n",
      "step 730, change in cost 1.80811\n",
      "step 740, training accuracy 0.249349\n",
      "step 740, cost 4593.06\n",
      "step 740, change in cost 1.78125\n",
      "step 750, training accuracy 0.249628\n",
      "step 750, cost 4591.31\n",
      "step 750, change in cost 1.75684\n",
      "step 760, training accuracy 0.249721\n",
      "step 760, cost 4589.58\n",
      "step 760, change in cost 1.72852\n",
      "step 770, training accuracy 0.249814\n",
      "step 770, cost 4587.87\n",
      "step 770, change in cost 1.70898\n",
      "step 780, training accuracy 0.249814\n",
      "step 780, cost 4586.18\n",
      "step 780, change in cost 1.68604\n",
      "step 790, training accuracy 0.250372\n",
      "step 790, cost 4584.53\n",
      "step 790, change in cost 1.65723\n",
      "step 800, training accuracy 0.250744\n",
      "step 800, cost 4582.89\n",
      "step 800, change in cost 1.63965\n",
      "step 810, training accuracy 0.251023\n",
      "step 810, cost 4581.27\n",
      "step 810, change in cost 1.61475\n",
      "step 820, training accuracy 0.251116\n",
      "step 820, cost 4579.67\n",
      "step 820, change in cost 1.59717\n",
      "step 830, training accuracy 0.251581\n",
      "step 830, cost 4578.1\n",
      "step 830, change in cost 1.56982\n",
      "step 840, training accuracy 0.251581\n",
      "step 840, cost 4576.55\n",
      "step 840, change in cost 1.55518\n",
      "step 850, training accuracy 0.251488\n",
      "step 850, cost 4575.02\n",
      "step 850, change in cost 1.52734\n",
      "step 860, training accuracy 0.251395\n",
      "step 860, cost 4573.51\n",
      "step 860, change in cost 1.50879\n",
      "step 870, training accuracy 0.251395\n",
      "step 870, cost 4572.02\n",
      "step 870, change in cost 1.49121\n",
      "step 880, training accuracy 0.251395\n",
      "step 880, cost 4570.56\n",
      "step 880, change in cost 1.46533\n",
      "step 890, training accuracy 0.251302\n",
      "step 890, cost 4569.11\n",
      "step 890, change in cost 1.44971\n",
      "step 900, training accuracy 0.251488\n",
      "step 900, cost 4567.67\n",
      "step 900, change in cost 1.43164\n",
      "step 910, training accuracy 0.251488\n",
      "step 910, cost 4566.27\n",
      "step 910, change in cost 1.40723\n",
      "step 920, training accuracy 0.251209\n",
      "step 920, cost 4564.88\n",
      "step 920, change in cost 1.38867\n",
      "step 930, training accuracy 0.251488\n",
      "step 930, cost 4563.51\n",
      "step 930, change in cost 1.36816\n",
      "step 940, training accuracy 0.251674\n",
      "step 940, cost 4562.16\n",
      "step 940, change in cost 1.34717\n",
      "step 950, training accuracy 0.25186\n",
      "step 950, cost 4560.83\n",
      "step 950, change in cost 1.33545\n",
      "step 960, training accuracy 0.251767\n",
      "step 960, cost 4559.52\n",
      "step 960, change in cost 1.3125\n",
      "step 970, training accuracy 0.252046\n",
      "step 970, cost 4558.22\n",
      "step 970, change in cost 1.29102\n",
      "step 980, training accuracy 0.252139\n",
      "step 980, cost 4556.95\n",
      "step 980, change in cost 1.27197\n",
      "step 990, training accuracy 0.252418\n",
      "step 990, cost 4555.7\n",
      "step 990, change in cost 1.25342\n",
      "step 1000, training accuracy 0.252232\n",
      "step 1000, cost 4554.46\n",
      "step 1000, change in cost 1.2373\n",
      "step 1010, training accuracy 0.252232\n",
      "step 1010, cost 4553.24\n",
      "step 1010, change in cost 1.2207\n",
      "step 1020, training accuracy 0.252232\n",
      "step 1020, cost 4552.04\n",
      "step 1020, change in cost 1.19922\n",
      "step 1030, training accuracy 0.252511\n",
      "step 1030, cost 4550.86\n",
      "step 1030, change in cost 1.1792\n",
      "step 1040, training accuracy 0.25279\n",
      "step 1040, cost 4549.7\n",
      "step 1040, change in cost 1.16748\n",
      "step 1050, training accuracy 0.253348\n",
      "step 1050, cost 4548.54\n",
      "step 1050, change in cost 1.15039\n",
      "step 1060, training accuracy 0.253534\n",
      "step 1060, cost 4547.41\n",
      "step 1060, change in cost 1.13574\n",
      "step 1070, training accuracy 0.253627\n",
      "step 1070, cost 4546.3\n",
      "step 1070, change in cost 1.11279\n",
      "step 1080, training accuracy 0.253813\n",
      "step 1080, cost 4545.2\n",
      "step 1080, change in cost 1.09814\n",
      "step 1090, training accuracy 0.253813\n",
      "step 1090, cost 4544.11\n",
      "step 1090, change in cost 1.08691\n",
      "step 1100, training accuracy 0.253999\n",
      "step 1100, cost 4543.04\n",
      "step 1100, change in cost 1.06689\n",
      "step 1110, training accuracy 0.254185\n",
      "step 1110, cost 4541.99\n",
      "step 1110, change in cost 1.05518\n",
      "step 1120, training accuracy 0.254278\n",
      "step 1120, cost 4540.95\n",
      "step 1120, change in cost 1.04297\n",
      "step 1130, training accuracy 0.254371\n",
      "step 1130, cost 4539.93\n",
      "step 1130, change in cost 1.02051\n",
      "step 1140, training accuracy 0.254278\n",
      "step 1140, cost 4538.92\n",
      "step 1140, change in cost 1.01025\n",
      "step 1150, training accuracy 0.254371\n",
      "step 1150, cost 4537.92\n",
      "step 1150, change in cost 0.998047\n",
      "step 1160, training accuracy 0.254371\n",
      "step 1160, cost 4536.93\n",
      "step 1160, change in cost 0.98291\n",
      "step 1170, training accuracy 0.254557\n",
      "step 1170, cost 4535.97\n",
      "step 1170, change in cost 0.966309\n",
      "step 1180, training accuracy 0.254743\n",
      "step 1180, cost 4535.01\n",
      "step 1180, change in cost 0.959473\n",
      "step 1190, training accuracy 0.254557\n",
      "step 1190, cost 4534.06\n",
      "step 1190, change in cost 0.950195\n",
      "step 1200, training accuracy 0.254464\n",
      "step 1200, cost 4533.13\n",
      "step 1200, change in cost 0.933105\n",
      "step 1210, training accuracy 0.254464\n",
      "step 1210, cost 4532.21\n",
      "step 1210, change in cost 0.917969\n",
      "step 1220, training accuracy 0.254743\n",
      "step 1220, cost 4531.3\n",
      "step 1220, change in cost 0.909668\n",
      "step 1230, training accuracy 0.254557\n",
      "step 1230, cost 4530.4\n",
      "step 1230, change in cost 0.899414\n",
      "step 1240, training accuracy 0.25465\n",
      "step 1240, cost 4529.51\n",
      "step 1240, change in cost 0.885742\n",
      "step 1250, training accuracy 0.254929\n",
      "step 1250, cost 4528.63\n",
      "step 1250, change in cost 0.881836\n",
      "step 1260, training accuracy 0.254929\n",
      "step 1260, cost 4527.77\n",
      "step 1260, change in cost 0.86084\n",
      "step 1270, training accuracy 0.255115\n",
      "step 1270, cost 4526.92\n",
      "step 1270, change in cost 0.853027\n",
      "step 1280, training accuracy 0.255115\n",
      "step 1280, cost 4526.07\n",
      "step 1280, change in cost 0.850098\n",
      "step 1290, training accuracy 0.255115\n",
      "step 1290, cost 4525.23\n",
      "step 1290, change in cost 0.833496\n",
      "step 1300, training accuracy 0.255301\n",
      "step 1300, cost 4524.41\n",
      "step 1300, change in cost 0.826172\n",
      "step 1310, training accuracy 0.255115\n",
      "step 1310, cost 4523.59\n",
      "step 1310, change in cost 0.81543\n",
      "step 1320, training accuracy 0.255394\n",
      "step 1320, cost 4522.78\n",
      "step 1320, change in cost 0.807617\n",
      "step 1330, training accuracy 0.255673\n",
      "step 1330, cost 4521.99\n",
      "step 1330, change in cost 0.798828\n",
      "step 1340, training accuracy 0.255766\n",
      "step 1340, cost 4521.2\n",
      "step 1340, change in cost 0.787109\n",
      "step 1350, training accuracy 0.255952\n",
      "step 1350, cost 4520.42\n",
      "step 1350, change in cost 0.782227\n",
      "step 1360, training accuracy 0.255952\n",
      "step 1360, cost 4519.64\n",
      "step 1360, change in cost 0.771973\n",
      "step 1370, training accuracy 0.255952\n",
      "step 1370, cost 4518.89\n",
      "step 1370, change in cost 0.757324\n",
      "step 1380, training accuracy 0.255766\n",
      "step 1380, cost 4518.13\n",
      "step 1380, change in cost 0.758789\n",
      "step 1390, training accuracy 0.256045\n",
      "step 1390, cost 4517.38\n",
      "step 1390, change in cost 0.743652\n",
      "step 1400, training accuracy 0.255766\n",
      "step 1400, cost 4516.65\n",
      "step 1400, change in cost 0.73877\n",
      "step 1410, training accuracy 0.255952\n",
      "step 1410, cost 4515.91\n",
      "step 1410, change in cost 0.733398\n",
      "step 1420, training accuracy 0.255952\n",
      "step 1420, cost 4515.19\n",
      "step 1420, change in cost 0.723145\n",
      "step 1430, training accuracy 0.256138\n",
      "step 1430, cost 4514.48\n",
      "step 1430, change in cost 0.712402\n",
      "step 1440, training accuracy 0.256138\n",
      "step 1440, cost 4513.77\n",
      "step 1440, change in cost 0.710938\n",
      "step 1450, training accuracy 0.256417\n",
      "step 1450, cost 4513.07\n",
      "step 1450, change in cost 0.699707\n",
      "step 1460, training accuracy 0.256417\n",
      "step 1460, cost 4512.37\n",
      "step 1460, change in cost 0.692871\n",
      "step 1470, training accuracy 0.256603\n",
      "step 1470, cost 4511.69\n",
      "step 1470, change in cost 0.687988\n",
      "step 1480, training accuracy 0.256696\n",
      "step 1480, cost 4511\n",
      "step 1480, change in cost 0.681641\n",
      "step 1490, training accuracy 0.256789\n",
      "step 1490, cost 4510.33\n",
      "step 1490, change in cost 0.675293\n",
      "step 1500, training accuracy 0.256789\n",
      "step 1500, cost 4509.67\n",
      "step 1500, change in cost 0.663086\n",
      "step 1510, training accuracy 0.256975\n",
      "step 1510, cost 4509\n",
      "step 1510, change in cost 0.665039\n",
      "step 1520, training accuracy 0.256975\n",
      "step 1520, cost 4508.35\n",
      "step 1520, change in cost 0.648438\n",
      "step 1530, training accuracy 0.256882\n",
      "step 1530, cost 4507.7\n",
      "step 1530, change in cost 0.647949\n",
      "step 1540, training accuracy 0.257347\n",
      "step 1540, cost 4507.06\n",
      "step 1540, change in cost 0.642578\n",
      "step 1550, training accuracy 0.257254\n",
      "step 1550, cost 4506.43\n",
      "step 1550, change in cost 0.632324\n",
      "step 1560, training accuracy 0.257068\n",
      "step 1560, cost 4505.8\n",
      "step 1560, change in cost 0.629883\n",
      "step 1570, training accuracy 0.257068\n",
      "step 1570, cost 4505.17\n",
      "step 1570, change in cost 0.625977\n",
      "step 1580, training accuracy 0.257347\n",
      "step 1580, cost 4504.55\n",
      "step 1580, change in cost 0.622559\n",
      "step 1590, training accuracy 0.25744\n",
      "step 1590, cost 4503.94\n",
      "step 1590, change in cost 0.61084\n",
      "step 1600, training accuracy 0.257254\n",
      "step 1600, cost 4503.34\n",
      "step 1600, change in cost 0.604004\n",
      "step 1610, training accuracy 0.257347\n",
      "step 1610, cost 4502.73\n",
      "step 1610, change in cost 0.603027\n",
      "step 1620, training accuracy 0.257161\n",
      "step 1620, cost 4502.14\n",
      "step 1620, change in cost 0.595215\n",
      "step 1630, training accuracy 0.257161\n",
      "step 1630, cost 4501.55\n",
      "step 1630, change in cost 0.59082\n",
      "step 1640, training accuracy 0.257161\n",
      "step 1640, cost 4500.96\n",
      "step 1640, change in cost 0.585938\n",
      "step 1650, training accuracy 0.257347\n",
      "step 1650, cost 4500.38\n",
      "step 1650, change in cost 0.580078\n",
      "step 1660, training accuracy 0.25744\n",
      "step 1660, cost 4499.81\n",
      "step 1660, change in cost 0.572754\n",
      "step 1670, training accuracy 0.25744\n",
      "step 1670, cost 4499.23\n",
      "step 1670, change in cost 0.576172\n",
      "step 1680, training accuracy 0.25744\n",
      "step 1680, cost 4498.67\n",
      "step 1680, change in cost 0.56543\n",
      "step 1690, training accuracy 0.25744\n",
      "step 1690, cost 4498.11\n",
      "step 1690, change in cost 0.560059\n",
      "step 1700, training accuracy 0.257626\n",
      "step 1700, cost 4497.55\n",
      "step 1700, change in cost 0.557617\n",
      "step 1710, training accuracy 0.25744\n",
      "step 1710, cost 4497\n",
      "step 1710, change in cost 0.550293\n",
      "step 1720, training accuracy 0.25744\n",
      "step 1720, cost 4496.45\n",
      "step 1720, change in cost 0.547363\n",
      "step 1730, training accuracy 0.25744\n",
      "step 1730, cost 4495.9\n",
      "step 1730, change in cost 0.546875\n",
      "step 1740, training accuracy 0.25744\n",
      "step 1740, cost 4495.37\n",
      "step 1740, change in cost 0.535645\n",
      "step 1750, training accuracy 0.25744\n",
      "step 1750, cost 4494.83\n",
      "step 1750, change in cost 0.535645\n",
      "step 1760, training accuracy 0.25744\n",
      "step 1760, cost 4494.31\n",
      "step 1760, change in cost 0.526855\n",
      "step 1770, training accuracy 0.257347\n",
      "step 1770, cost 4493.78\n",
      "step 1770, change in cost 0.525879\n",
      "step 1780, training accuracy 0.25744\n",
      "step 1780, cost 4493.26\n",
      "step 1780, change in cost 0.52002\n",
      "step 1790, training accuracy 0.257533\n",
      "step 1790, cost 4492.74\n",
      "step 1790, change in cost 0.520996\n",
      "step 1800, training accuracy 0.257626\n",
      "step 1800, cost 4492.23\n",
      "step 1800, change in cost 0.513184\n",
      "step 1810, training accuracy 0.25744\n",
      "step 1810, cost 4491.72\n",
      "step 1810, change in cost 0.505371\n",
      "step 1820, training accuracy 0.257533\n",
      "step 1820, cost 4491.21\n",
      "step 1820, change in cost 0.508301\n",
      "step 1830, training accuracy 0.257347\n",
      "step 1830, cost 4490.71\n",
      "step 1830, change in cost 0.500488\n",
      "step 1840, training accuracy 0.25744\n",
      "step 1840, cost 4490.22\n",
      "step 1840, change in cost 0.491699\n",
      "step 1850, training accuracy 0.257347\n",
      "step 1850, cost 4489.73\n",
      "step 1850, change in cost 0.493164\n",
      "step 1860, training accuracy 0.25744\n",
      "step 1860, cost 4489.24\n",
      "step 1860, change in cost 0.487305\n",
      "step 1870, training accuracy 0.257347\n",
      "step 1870, cost 4488.76\n",
      "step 1870, change in cost 0.480469\n",
      "step 1880, training accuracy 0.25744\n",
      "step 1880, cost 4488.28\n",
      "step 1880, change in cost 0.482422\n",
      "step 1890, training accuracy 0.257068\n",
      "step 1890, cost 4487.8\n",
      "step 1890, change in cost 0.475586\n",
      "step 1900, training accuracy 0.257068\n",
      "step 1900, cost 4487.33\n",
      "step 1900, change in cost 0.468262\n",
      "step 1910, training accuracy 0.257068\n",
      "step 1910, cost 4486.87\n",
      "step 1910, change in cost 0.46582\n",
      "step 1920, training accuracy 0.257068\n",
      "step 1920, cost 4486.41\n",
      "step 1920, change in cost 0.460449\n",
      "step 1930, training accuracy 0.257533\n",
      "step 1930, cost 4485.95\n",
      "step 1930, change in cost 0.458008\n",
      "step 1940, training accuracy 0.257812\n",
      "step 1940, cost 4485.49\n",
      "step 1940, change in cost 0.459473\n",
      "step 1950, training accuracy 0.257812\n",
      "step 1950, cost 4485.05\n",
      "step 1950, change in cost 0.441895\n",
      "step 1960, training accuracy 0.258092\n",
      "step 1960, cost 4484.6\n",
      "step 1960, change in cost 0.446777\n",
      "step 1970, training accuracy 0.258278\n",
      "step 1970, cost 4484.16\n",
      "step 1970, change in cost 0.436523\n",
      "step 1980, training accuracy 0.258278\n",
      "step 1980, cost 4483.72\n",
      "step 1980, change in cost 0.441895\n",
      "step 1990, training accuracy 0.258557\n",
      "step 1990, cost 4483.29\n",
      "step 1990, change in cost 0.43457\n",
      "final accuracy on test set: 0.239211\n"
     ]
    }
   ],
   "source": [
    "# tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "correct_predictions_OP = tf.equal(tf.argmax(activation_OP,1),tf.argmax(Y,1))\n",
    "accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "\n",
    "\n",
    "cost = 0\n",
    "diff = 1\n",
    "epoch_values=[]\n",
    "accuracy_values=[]\n",
    "cost_values=[]\n",
    "\n",
    "# Training epochs\n",
    "for i in range(numEpochs):\n",
    "    if i > 1 and diff < .0001:\n",
    "        print(\"change in cost %g; convergence.\"%diff)\n",
    "        break\n",
    "    else:\n",
    "        # Run training step\n",
    "        step = sess.run(training_OP, feed_dict={X: train_data, Y: train_label})\n",
    "        # Report occasional stats\n",
    "        if i % 10 == 0:\n",
    "            # Add epoch to epoch_values\n",
    "            epoch_values.append(i)\n",
    "            # Generate accuracy stats on test data\n",
    "            train_accuracy, newCost = sess.run(\n",
    "                [accuracy_OP, cost_OP], \n",
    "                feed_dict={X: train_data, Y: train_label}\n",
    "            )\n",
    "            # Add accuracy to live graphing variable\n",
    "            accuracy_values.append(train_accuracy)\n",
    "            # Add cost to live graphing variable\n",
    "            cost_values.append(newCost)\n",
    "            \n",
    "            # Re-assign values for variables\n",
    "            diff = abs(newCost - cost)\n",
    "            cost = newCost\n",
    "\n",
    "            #generate print statements\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, cost %g\"%(i, newCost))\n",
    "            print(\"step %d, change in cost %g\"%(i, diff))\n",
    "\n",
    "            \n",
    "\n",
    "# How well do we perform on held-out test data?\n",
    "print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP, \n",
    "                                                     feed_dict={X: testX, \n",
    "                                                                Y: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
