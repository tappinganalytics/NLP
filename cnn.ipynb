{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from sklearn import preprocessing \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from __future__ import division\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"data_content\", header=None, skip_blank_lines=False)\n",
    "label = pd.read_table(\"label\", header=None, dtype='category', skip_blank_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train.columns = ['question']\n",
    "label.columns = ['group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67213, 1)\n",
      "(67213, 1)\n"
     ]
    }
   ],
   "source": [
    "print train.shape\n",
    "print label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio = 0.2\n",
    "# Separate each muscle group data\n",
    "chest = train[label['group']=='Chest']\n",
    "chest_label = label[label['group']=='Chest']\n",
    "chest_len = int(chest.shape[0]*ratio)\n",
    "\n",
    "shoulder = train[label['group']=='Shoulders']\n",
    "shoulder_len = int(shoulder.shape[0]*ratio)\n",
    "shoulder_label = label[label['group']=='Shoulders']\n",
    "\n",
    "back = train[label['group']=='Back']\n",
    "back_label = label[label['group']=='Back']\n",
    "back_len = int(back.shape[0]*ratio)\n",
    "\n",
    "leg = train[label['group']=='Leg']\n",
    "leg_label = label[label['group']=='Leg']\n",
    "leg_len = int(leg.shape[0]*ratio)\n",
    "\n",
    "tricep = train[label['group']=='Triceps']\n",
    "tricep_label = label[label['group']=='Triceps']\n",
    "tricep_len = int(tricep.shape[0]*ratio)\n",
    "\n",
    "bicep = train[label['group']=='Biceps']\n",
    "bicep_label = label[label['group']=='Biceps']\n",
    "bicep_len = int(bicep.shape[0]*ratio)\n",
    "\n",
    "ab = train[label['group']=='Abs']\n",
    "ab_label = label[label['group']=='Abs']\n",
    "ab_len = int(ab.shape[0]*ratio)\n",
    "\n",
    "glute = train[label['group']=='Glutes']\n",
    "glute_label = label[label['group']=='Glutes']\n",
    "glute_len = int(glute.shape[0]*ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13440, 1)\n",
      "(13440, 1)\n"
     ]
    }
   ],
   "source": [
    "# Take only the top ratio% of each group of data\n",
    "train_data = pd.concat([chest[0:chest_len], shoulder[0:shoulder_len], back[0:back_len], \n",
    "                        leg[0:leg_len], tricep[0:tricep_len], bicep[0:bicep_len], ab[0:ab_len], glute[0: glute_len]])\n",
    "print train_data.shape\n",
    "train_label = pd.concat([chest_label[0:chest_len], shoulder_label[0:shoulder_len], back_label[0:back_len], \n",
    "                   leg_label[0:leg_len], tricep_label[0:tricep_len], bicep_label[0:bicep_len], ab_label[0:ab_len],\n",
    "                         glute_label[0: glute_len]])\n",
    "print train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_label = pd.Series(train_label['group'],dtype='category')\n",
    "y_label.cat.categories \n",
    "y_label.cat.categories = [0,1,2,3,4,5,6,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#label binizer\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "lb.fit([0,1,2,3,4,5,6,7])\n",
    "label = lb.transform(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#initialize TFIDF vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),stop_words=\"english\")\n",
    "data = vectorizer.fit_transform(train_data['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduce features using SVD\n",
    "SVD = TruncatedSVD(n_components=100, n_iter=5, random_state=0)\n",
    "train = SVD.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10752, 100)\n",
      "(10752, 8)\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "train_data, testX, train_label, testY = train_test_split(train, label, test_size=0.2, random_state=0)\n",
    "print train_data.shape\n",
    "print train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow setup\n",
    "numFeatures = train_data.shape[1]\n",
    "numLabels = 8\n",
    "numEpochs = 1500\n",
    "learningRate = tf.train.exponential_decay(learning_rate=0.005,\n",
    "                                          global_step= 1,\n",
    "                                          decay_steps=train_data.shape[0],\n",
    "                                          decay_rate= 0.9,\n",
    "                                          staircase=True)\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, numFeatures])\n",
    "Y = tf.placeholder(tf.float32, [None, numLabels])\n",
    "\n",
    "weights = tf.Variable(tf.random_normal([numFeatures,numLabels],\n",
    "                                       mean=0,\n",
    "                                       stddev=(np.sqrt(6/numFeatures+\n",
    "                                                         numLabels+1)),\n",
    "                                       name=\"weights\"))\n",
    "\n",
    "bias = tf.Variable(tf.random_normal([1,numLabels],\n",
    "                                    mean=0,\n",
    "                                    stddev=(np.sqrt(6/numFeatures+numLabels+1)),\n",
    "                                    name=\"bias\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tensorflow operation\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "apply_weights_OP = tf.matmul(X, weights, name=\"apply_weights\")\n",
    "add_bias_OP = tf.add(apply_weights_OP, bias, name=\"add_bias\") \n",
    "activation_OP = tf.nn.sigmoid(add_bias_OP, name=\"activation\")\n",
    "\n",
    "cost_OP = tf.nn.l2_loss(activation_OP-Y, name=\"squared_error_cost\")\n",
    "training_OP = tf.train.GradientDescentOptimizer(learningRate).minimize(cost_OP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.132533\n",
      "step 0, cost 12096.5\n",
      "step 0, change in cost 12096.5\n",
      "step 10, training accuracy 0.132906\n",
      "step 10, cost 8943.58\n",
      "step 10, change in cost 3152.89\n",
      "step 20, training accuracy 0.132999\n",
      "step 20, cost 8894.1\n",
      "step 20, change in cost 49.4756\n",
      "step 30, training accuracy 0.132999\n",
      "step 30, cost 8741.26\n",
      "step 30, change in cost 152.836\n",
      "step 40, training accuracy 0.236142\n",
      "step 40, cost 4914.18\n",
      "step 40, change in cost 3827.08\n",
      "step 50, training accuracy 0.236328\n",
      "step 50, cost 4891.97\n",
      "step 50, change in cost 22.2173\n",
      "step 60, training accuracy 0.236328\n",
      "step 60, cost 4877.46\n",
      "step 60, change in cost 14.5117\n",
      "step 70, training accuracy 0.237444\n",
      "step 70, cost 4865.09\n",
      "step 70, change in cost 12.3613\n",
      "step 80, training accuracy 0.238746\n",
      "step 80, cost 4853.27\n",
      "step 80, change in cost 11.8271\n",
      "step 90, training accuracy 0.240513\n",
      "step 90, cost 4841.94\n",
      "step 90, change in cost 11.3291\n",
      "step 100, training accuracy 0.241629\n",
      "step 100, cost 4831.11\n",
      "step 100, change in cost 10.8291\n",
      "step 110, training accuracy 0.242188\n",
      "step 110, cost 4820.77\n",
      "step 110, change in cost 10.3364\n",
      "step 120, training accuracy 0.242374\n",
      "step 120, cost 4810.94\n",
      "step 120, change in cost 9.83398\n",
      "step 130, training accuracy 0.243211\n",
      "step 130, cost 4801.6\n",
      "step 130, change in cost 9.34277\n",
      "step 140, training accuracy 0.244141\n",
      "step 140, cost 4792.74\n",
      "step 140, change in cost 8.85791\n",
      "step 150, training accuracy 0.244234\n",
      "step 150, cost 4784.35\n",
      "step 150, change in cost 8.3833\n",
      "step 160, training accuracy 0.244792\n",
      "step 160, cost 4776.42\n",
      "step 160, change in cost 7.93652\n",
      "step 170, training accuracy 0.24535\n",
      "step 170, cost 4768.91\n",
      "step 170, change in cost 7.50586\n",
      "step 180, training accuracy 0.245629\n",
      "step 180, cost 4761.82\n",
      "step 180, change in cost 7.09277\n",
      "step 190, training accuracy 0.246094\n",
      "step 190, cost 4755.1\n",
      "step 190, change in cost 6.71875\n",
      "step 200, training accuracy 0.246652\n",
      "step 200, cost 4748.74\n",
      "step 200, change in cost 6.36133\n",
      "step 210, training accuracy 0.246559\n",
      "step 210, cost 4742.71\n",
      "step 210, change in cost 6.02686\n",
      "step 220, training accuracy 0.246838\n",
      "step 220, cost 4737\n",
      "step 220, change in cost 5.71729\n",
      "step 230, training accuracy 0.247117\n",
      "step 230, cost 4731.56\n",
      "step 230, change in cost 5.43066\n",
      "step 240, training accuracy 0.247024\n",
      "step 240, cost 4726.4\n",
      "step 240, change in cost 5.16553\n",
      "step 250, training accuracy 0.247582\n",
      "step 250, cost 4721.48\n",
      "step 250, change in cost 4.91797\n",
      "step 260, training accuracy 0.247861\n",
      "step 260, cost 4716.79\n",
      "step 260, change in cost 4.69385\n",
      "step 270, training accuracy 0.248047\n",
      "step 270, cost 4712.31\n",
      "step 270, change in cost 4.47656\n",
      "step 280, training accuracy 0.24814\n",
      "step 280, cost 4708.03\n",
      "step 280, change in cost 4.27881\n",
      "step 290, training accuracy 0.248047\n",
      "step 290, cost 4703.93\n",
      "step 290, change in cost 4.09961\n",
      "step 300, training accuracy 0.248419\n",
      "step 300, cost 4700.01\n",
      "step 300, change in cost 3.92383\n",
      "step 310, training accuracy 0.248791\n",
      "step 310, cost 4696.24\n",
      "step 310, change in cost 3.76904\n",
      "step 320, training accuracy 0.248698\n",
      "step 320, cost 4692.62\n",
      "step 320, change in cost 3.62012\n",
      "step 330, training accuracy 0.248884\n",
      "step 330, cost 4689.14\n",
      "step 330, change in cost 3.48242\n",
      "step 340, training accuracy 0.248605\n",
      "step 340, cost 4685.78\n",
      "step 340, change in cost 3.35742\n",
      "step 350, training accuracy 0.248698\n",
      "step 350, cost 4682.54\n",
      "step 350, change in cost 3.24219\n",
      "step 360, training accuracy 0.248512\n",
      "step 360, cost 4679.41\n",
      "step 360, change in cost 3.12695\n",
      "step 370, training accuracy 0.248605\n",
      "step 370, cost 4676.39\n",
      "step 370, change in cost 3.0249\n",
      "step 380, training accuracy 0.248605\n",
      "step 380, cost 4673.46\n",
      "step 380, change in cost 2.92578\n",
      "step 390, training accuracy 0.248512\n",
      "step 390, cost 4670.62\n",
      "step 390, change in cost 2.83838\n",
      "step 400, training accuracy 0.248512\n",
      "step 400, cost 4667.87\n",
      "step 400, change in cost 2.75049\n",
      "step 410, training accuracy 0.248884\n",
      "step 410, cost 4665.19\n",
      "step 410, change in cost 2.67627\n",
      "step 420, training accuracy 0.249163\n",
      "step 420, cost 4662.6\n",
      "step 420, change in cost 2.59912\n",
      "step 430, training accuracy 0.249256\n",
      "step 430, cost 4660.06\n",
      "step 430, change in cost 2.53223\n",
      "step 440, training accuracy 0.249163\n",
      "step 440, cost 4657.6\n",
      "step 440, change in cost 2.46338\n",
      "step 450, training accuracy 0.24907\n",
      "step 450, cost 4655.19\n",
      "step 450, change in cost 2.4082\n",
      "step 460, training accuracy 0.249163\n",
      "step 460, cost 4652.85\n",
      "step 460, change in cost 2.34473\n",
      "step 470, training accuracy 0.249163\n",
      "step 470, cost 4650.56\n",
      "step 470, change in cost 2.28955\n",
      "step 480, training accuracy 0.249349\n",
      "step 480, cost 4648.32\n",
      "step 480, change in cost 2.23877\n",
      "step 490, training accuracy 0.249256\n",
      "step 490, cost 4646.13\n",
      "step 490, change in cost 2.19238\n",
      "step 500, training accuracy 0.24907\n",
      "step 500, cost 4643.98\n",
      "step 500, change in cost 2.14355\n",
      "step 510, training accuracy 0.249256\n",
      "step 510, cost 4641.88\n",
      "step 510, change in cost 2.10449\n",
      "step 520, training accuracy 0.249256\n",
      "step 520, cost 4639.82\n",
      "step 520, change in cost 2.06006\n",
      "step 530, training accuracy 0.249535\n",
      "step 530, cost 4637.8\n",
      "step 530, change in cost 2.02197\n",
      "step 540, training accuracy 0.249721\n",
      "step 540, cost 4635.82\n",
      "step 540, change in cost 1.98047\n",
      "step 550, training accuracy 0.249814\n",
      "step 550, cost 4633.87\n",
      "step 550, change in cost 1.94824\n",
      "step 560, training accuracy 0.250279\n",
      "step 560, cost 4631.96\n",
      "step 560, change in cost 1.90967\n",
      "step 570, training accuracy 0.250093\n",
      "step 570, cost 4630.07\n",
      "step 570, change in cost 1.88623\n",
      "step 580, training accuracy 0.250186\n",
      "step 580, cost 4628.22\n",
      "step 580, change in cost 1.85156\n",
      "step 590, training accuracy 0.250651\n",
      "step 590, cost 4626.4\n",
      "step 590, change in cost 1.82324\n",
      "step 600, training accuracy 0.250558\n",
      "step 600, cost 4624.61\n",
      "step 600, change in cost 1.78809\n",
      "step 610, training accuracy 0.250372\n",
      "step 610, cost 4622.84\n",
      "step 610, change in cost 1.77246\n",
      "step 620, training accuracy 0.250372\n",
      "step 620, cost 4621.1\n",
      "step 620, change in cost 1.73633\n",
      "step 630, training accuracy 0.250558\n",
      "step 630, cost 4619.38\n",
      "step 630, change in cost 1.71582\n",
      "step 640, training accuracy 0.250279\n",
      "step 640, cost 4617.69\n",
      "step 640, change in cost 1.69482\n",
      "step 650, training accuracy 0.250279\n",
      "step 650, cost 4616.02\n",
      "step 650, change in cost 1.6665\n",
      "step 660, training accuracy 0.250279\n",
      "step 660, cost 4614.37\n",
      "step 660, change in cost 1.6499\n",
      "step 670, training accuracy 0.250372\n",
      "step 670, cost 4612.75\n",
      "step 670, change in cost 1.62402\n",
      "step 680, training accuracy 0.250651\n",
      "step 680, cost 4611.15\n",
      "step 680, change in cost 1.60205\n",
      "step 690, training accuracy 0.250558\n",
      "step 690, cost 4609.56\n",
      "step 690, change in cost 1.58789\n",
      "step 700, training accuracy 0.250186\n",
      "step 700, cost 4608\n",
      "step 700, change in cost 1.56299\n",
      "step 710, training accuracy 0.250279\n",
      "step 710, cost 4606.45\n",
      "step 710, change in cost 1.54932\n",
      "step 720, training accuracy 0.250372\n",
      "step 720, cost 4604.91\n",
      "step 720, change in cost 1.53223\n",
      "step 730, training accuracy 0.250837\n",
      "step 730, cost 4603.4\n",
      "step 730, change in cost 1.51514\n",
      "step 740, training accuracy 0.251023\n",
      "step 740, cost 4601.91\n",
      "step 740, change in cost 1.49365\n",
      "step 750, training accuracy 0.251023\n",
      "step 750, cost 4600.43\n",
      "step 750, change in cost 1.47656\n",
      "step 760, training accuracy 0.251209\n",
      "step 760, cost 4598.97\n",
      "step 760, change in cost 1.46289\n",
      "step 770, training accuracy 0.251395\n",
      "step 770, cost 4597.52\n",
      "step 770, change in cost 1.4502\n",
      "step 780, training accuracy 0.251674\n",
      "step 780, cost 4596.08\n",
      "step 780, change in cost 1.43262\n",
      "step 790, training accuracy 0.251953\n",
      "step 790, cost 4594.66\n",
      "step 790, change in cost 1.41992\n",
      "step 800, training accuracy 0.252418\n",
      "step 800, cost 4593.26\n",
      "step 800, change in cost 1.4043\n",
      "step 810, training accuracy 0.252511\n",
      "step 810, cost 4591.87\n",
      "step 810, change in cost 1.3916\n",
      "step 820, training accuracy 0.252604\n",
      "step 820, cost 4590.49\n",
      "step 820, change in cost 1.38184\n",
      "step 830, training accuracy 0.252418\n",
      "step 830, cost 4589.13\n",
      "step 830, change in cost 1.35938\n",
      "step 840, training accuracy 0.252418\n",
      "step 840, cost 4587.77\n",
      "step 840, change in cost 1.354\n",
      "step 850, training accuracy 0.252697\n",
      "step 850, cost 4586.43\n",
      "step 850, change in cost 1.33984\n",
      "step 860, training accuracy 0.252604\n",
      "step 860, cost 4585.11\n",
      "step 860, change in cost 1.32129\n",
      "step 870, training accuracy 0.252604\n",
      "step 870, cost 4583.8\n",
      "step 870, change in cost 1.31494\n",
      "step 880, training accuracy 0.252976\n",
      "step 880, cost 4582.49\n",
      "step 880, change in cost 1.31055\n",
      "step 890, training accuracy 0.252883\n",
      "step 890, cost 4581.2\n",
      "step 890, change in cost 1.2876\n",
      "step 900, training accuracy 0.252883\n",
      "step 900, cost 4579.92\n",
      "step 900, change in cost 1.27783\n",
      "step 910, training accuracy 0.253348\n",
      "step 910, cost 4578.65\n",
      "step 910, change in cost 1.26562\n",
      "step 920, training accuracy 0.25372\n",
      "step 920, cost 4577.39\n",
      "step 920, change in cost 1.26123\n",
      "step 930, training accuracy 0.253813\n",
      "step 930, cost 4576.15\n",
      "step 930, change in cost 1.24463\n",
      "step 940, training accuracy 0.254092\n",
      "step 940, cost 4574.91\n",
      "step 940, change in cost 1.23828\n",
      "step 950, training accuracy 0.254371\n",
      "step 950, cost 4573.69\n",
      "step 950, change in cost 1.21875\n",
      "step 960, training accuracy 0.254092\n",
      "step 960, cost 4572.47\n",
      "step 960, change in cost 1.21924\n",
      "step 970, training accuracy 0.254278\n",
      "step 970, cost 4571.27\n",
      "step 970, change in cost 1.19922\n",
      "step 980, training accuracy 0.254278\n",
      "step 980, cost 4570.08\n",
      "step 980, change in cost 1.19141\n",
      "step 990, training accuracy 0.254464\n",
      "step 990, cost 4568.89\n",
      "step 990, change in cost 1.18896\n",
      "step 1000, training accuracy 0.254836\n",
      "step 1000, cost 4567.72\n",
      "step 1000, change in cost 1.1709\n",
      "step 1010, training accuracy 0.255115\n",
      "step 1010, cost 4566.56\n",
      "step 1010, change in cost 1.16113\n",
      "step 1020, training accuracy 0.255022\n",
      "step 1020, cost 4565.41\n",
      "step 1020, change in cost 1.1543\n",
      "step 1030, training accuracy 0.255022\n",
      "step 1030, cost 4564.26\n",
      "step 1030, change in cost 1.14355\n",
      "step 1040, training accuracy 0.255208\n",
      "step 1040, cost 4563.13\n",
      "step 1040, change in cost 1.12988\n",
      "step 1050, training accuracy 0.255394\n",
      "step 1050, cost 4562.01\n",
      "step 1050, change in cost 1.12207\n",
      "step 1060, training accuracy 0.255487\n",
      "step 1060, cost 4560.9\n",
      "step 1060, change in cost 1.11426\n",
      "step 1070, training accuracy 0.255673\n",
      "step 1070, cost 4559.8\n",
      "step 1070, change in cost 1.10107\n",
      "step 1080, training accuracy 0.256045\n",
      "step 1080, cost 4558.7\n",
      "step 1080, change in cost 1.09229\n",
      "step 1090, training accuracy 0.256324\n",
      "step 1090, cost 4557.62\n",
      "step 1090, change in cost 1.08154\n",
      "step 1100, training accuracy 0.256417\n",
      "step 1100, cost 4556.55\n",
      "step 1100, change in cost 1.07227\n",
      "step 1110, training accuracy 0.256417\n",
      "step 1110, cost 4555.49\n",
      "step 1110, change in cost 1.05811\n",
      "step 1120, training accuracy 0.256603\n",
      "step 1120, cost 4554.44\n",
      "step 1120, change in cost 1.05225\n",
      "step 1130, training accuracy 0.256417\n",
      "step 1130, cost 4553.4\n",
      "step 1130, change in cost 1.04053\n",
      "step 1140, training accuracy 0.256789\n",
      "step 1140, cost 4552.37\n",
      "step 1140, change in cost 1.02881\n",
      "step 1150, training accuracy 0.256696\n",
      "step 1150, cost 4551.34\n",
      "step 1150, change in cost 1.02588\n",
      "step 1160, training accuracy 0.256975\n",
      "step 1160, cost 4550.34\n",
      "step 1160, change in cost 1.0083\n",
      "step 1170, training accuracy 0.257068\n",
      "step 1170, cost 4549.34\n",
      "step 1170, change in cost 0.998535\n",
      "step 1180, training accuracy 0.257254\n",
      "step 1180, cost 4548.35\n",
      "step 1180, change in cost 0.991211\n",
      "step 1190, training accuracy 0.257161\n",
      "step 1190, cost 4547.36\n",
      "step 1190, change in cost 0.985352\n",
      "step 1200, training accuracy 0.257068\n",
      "step 1200, cost 4546.39\n",
      "step 1200, change in cost 0.969727\n",
      "step 1210, training accuracy 0.256975\n",
      "step 1210, cost 4545.43\n",
      "step 1210, change in cost 0.960938\n",
      "step 1220, training accuracy 0.257347\n",
      "step 1220, cost 4544.48\n",
      "step 1220, change in cost 0.948242\n",
      "step 1230, training accuracy 0.257347\n",
      "step 1230, cost 4543.54\n",
      "step 1230, change in cost 0.941895\n",
      "step 1240, training accuracy 0.257347\n",
      "step 1240, cost 4542.61\n",
      "step 1240, change in cost 0.932129\n",
      "step 1250, training accuracy 0.257347\n",
      "step 1250, cost 4541.69\n",
      "step 1250, change in cost 0.918945\n",
      "step 1260, training accuracy 0.25744\n",
      "step 1260, cost 4540.77\n",
      "step 1260, change in cost 0.914062\n",
      "step 1270, training accuracy 0.257347\n",
      "step 1270, cost 4539.88\n",
      "step 1270, change in cost 0.898438\n",
      "step 1280, training accuracy 0.257161\n",
      "step 1280, cost 4538.98\n",
      "step 1280, change in cost 0.891602\n",
      "step 1290, training accuracy 0.257347\n",
      "step 1290, cost 4538.11\n",
      "step 1290, change in cost 0.878906\n",
      "step 1300, training accuracy 0.257254\n",
      "step 1300, cost 4537.23\n",
      "step 1300, change in cost 0.880371\n",
      "step 1310, training accuracy 0.257254\n",
      "step 1310, cost 4536.37\n",
      "step 1310, change in cost 0.857422\n",
      "step 1320, training accuracy 0.257254\n",
      "step 1320, cost 4535.51\n",
      "step 1320, change in cost 0.853027\n",
      "step 1330, training accuracy 0.256975\n",
      "step 1330, cost 4534.67\n",
      "step 1330, change in cost 0.847168\n",
      "step 1340, training accuracy 0.257161\n",
      "step 1340, cost 4533.83\n",
      "step 1340, change in cost 0.839355\n",
      "step 1350, training accuracy 0.257347\n",
      "step 1350, cost 4533\n",
      "step 1350, change in cost 0.823242\n",
      "step 1360, training accuracy 0.25744\n",
      "step 1360, cost 4532.19\n",
      "step 1360, change in cost 0.814453\n",
      "step 1370, training accuracy 0.257347\n",
      "step 1370, cost 4531.38\n",
      "step 1370, change in cost 0.807617\n",
      "step 1380, training accuracy 0.257533\n",
      "step 1380, cost 4530.58\n",
      "step 1380, change in cost 0.80127\n",
      "step 1390, training accuracy 0.257626\n",
      "step 1390, cost 4529.79\n",
      "step 1390, change in cost 0.791504\n",
      "step 1400, training accuracy 0.257999\n",
      "step 1400, cost 4529.01\n",
      "step 1400, change in cost 0.782715\n",
      "step 1410, training accuracy 0.258185\n",
      "step 1410, cost 4528.24\n",
      "step 1410, change in cost 0.769531\n",
      "step 1420, training accuracy 0.258185\n",
      "step 1420, cost 4527.47\n",
      "step 1420, change in cost 0.768066\n",
      "step 1430, training accuracy 0.258278\n",
      "step 1430, cost 4526.71\n",
      "step 1430, change in cost 0.758301\n",
      "step 1440, training accuracy 0.258278\n",
      "step 1440, cost 4525.96\n",
      "step 1440, change in cost 0.749023\n",
      "step 1450, training accuracy 0.258371\n",
      "step 1450, cost 4525.22\n",
      "step 1450, change in cost 0.737793\n",
      "step 1460, training accuracy 0.258371\n",
      "step 1460, cost 4524.49\n",
      "step 1460, change in cost 0.732422\n",
      "step 1470, training accuracy 0.258464\n",
      "step 1470, cost 4523.76\n",
      "step 1470, change in cost 0.730469\n",
      "step 1480, training accuracy 0.258743\n",
      "step 1480, cost 4523.05\n",
      "step 1480, change in cost 0.716309\n",
      "step 1490, training accuracy 0.259208\n",
      "step 1490, cost 4522.33\n",
      "step 1490, change in cost 0.711426\n",
      "final accuracy on test set: 0.229167\n"
     ]
    }
   ],
   "source": [
    "# tensorflow session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "correct_predictions_OP = tf.equal(tf.argmax(activation_OP,1),tf.argmax(Y,1))\n",
    "accuracy_OP = tf.reduce_mean(tf.cast(correct_predictions_OP, \"float\"))\n",
    "\n",
    "\n",
    "cost = 0\n",
    "diff = 1\n",
    "epoch_values=[]\n",
    "accuracy_values=[]\n",
    "cost_values=[]\n",
    "\n",
    "# Training epochs\n",
    "for i in range(numEpochs):\n",
    "    if i > 1 and diff < .0001:\n",
    "        print(\"change in cost %g; convergence.\"%diff)\n",
    "        break\n",
    "    else:\n",
    "        # Run training step\n",
    "        step = sess.run(training_OP, feed_dict={X: train_data, Y: train_label})\n",
    "        # Report occasional stats\n",
    "        if i % 10 == 0:\n",
    "            # Add epoch to epoch_values\n",
    "            epoch_values.append(i)\n",
    "            # Generate accuracy stats on test data\n",
    "            train_accuracy, newCost = sess.run(\n",
    "                [accuracy_OP, cost_OP], \n",
    "                feed_dict={X: train_data, Y: train_label}\n",
    "            )\n",
    "            # Add accuracy to live graphing variable\n",
    "            accuracy_values.append(train_accuracy)\n",
    "            # Add cost to live graphing variable\n",
    "            cost_values.append(newCost)\n",
    "            \n",
    "            # Re-assign values for variables\n",
    "            diff = abs(newCost - cost)\n",
    "            cost = newCost\n",
    "\n",
    "            #generate print statements\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "            print(\"step %d, cost %g\"%(i, newCost))\n",
    "            print(\"step %d, change in cost %g\"%(i, diff))\n",
    "\n",
    "            \n",
    "\n",
    "# How well do we perform on held-out test data?\n",
    "print(\"final accuracy on test set: %s\" %str(sess.run(accuracy_OP, \n",
    "                                                     feed_dict={X: testX, \n",
    "                                                                Y: testY})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
